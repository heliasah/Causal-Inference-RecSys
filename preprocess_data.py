# -*- coding: utf-8 -*-
"""Preprocess Data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1toE1aZ0O7C9SkaBhmgNCyaALMaPC5_bQ
"""

!pip install pandas scikit-learn

import pandas as pd
from sklearn.model_selection import train_test_split

!wget http://files.grouplens.org/datasets/movielens/ml-100k.zip
!unzip ml-100k.zip

ratings = pd.read_csv(
    'ml-100k/u.data',
    sep='\t',
    header=None,
    names=['user_id', 'movie_id', 'rating', 'timestamp']
)

print("First 5 rows:")
print(ratings.head())
print("\nDataset info:")
print(ratings.info())
print("\nRating statistics:")
print(ratings['rating'].describe())

print("\nMissing values:")
print(ratings.isnull().sum())

ratings['user_id'] = ratings['user_id'].astype('category').cat.codes
ratings['movie_id'] = ratings['movie_id'].astype('category').cat.codes

train_df, test_df = train_test_split(
    ratings[['user_id', 'movie_id', 'rating']],
    test_size=0.2,
    random_state=42
)

train_df.to_csv('train_ratings.csv', index=False)
test_df.to_csv('test_ratings.csv', index=False)

print("\nPreprocessing completed!")
print(f"Training samples: {len(train_df)}")
print(f"Test samples: {len(test_df)}")

import numpy as np
import pandas as pd

train_df = pd.read_csv('train_ratings.csv')
test_df = pd.read_csv('test_ratings.csv')

num_p = max(train_df['user_id'].max(), test_df['user_id'].max()) + 1
num_m = max(train_df['movie_id'].max(), test_df['movie_id'].max()) + 1

count = np.zeros((num_p, num_m), dtype=np.float32)

for movie_id, group in train_df.groupby('movie_id'):
    user_ids = group['user_id'].values
    ratings = group['rating'].values
    count[user_ids, movie_id] = ratings

print(f"Created rating matrix with shape: {count.shape}")
print(f"Non-zero entries: {np.count_nonzero(count)}")

"""# Preprocess
The code downloads the MovieLens 100K dataset, extracts the file, and loads the ratings data into a pandas DataFrame. It explores the dataset by displaying sample rows, dataset info, summary statistics, and checking for missing values. The user and movie IDs are converted to a continuous index (0-based) for easier processing. The dataset is then split into 80% training and 20% testing subsets, which are saved as CSV files. The second part of the code loads the preprocessed training and test data, determines the number of unique users and movies, and constructs a user-item rating matrix where each row represents a user and each column represents a movie, filling in the matrix with available ratings. Finally, it prints the matrix dimensions and the number of non-zero ratings.
"""