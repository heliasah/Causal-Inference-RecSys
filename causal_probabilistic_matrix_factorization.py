# -*- coding: utf-8 -*-
"""Causal Probabilistic Matrix Factorization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1toE1aZ0O7C9SkaBhmgNCyaALMaPC5_bQ
"""

import numpy as np
import pandas as pd
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

train_df = pd.read_csv('train_ratings.csv')
test_df = pd.read_csv('test_ratings.csv')

train_data = train_df[['user_id', 'movie_id', 'rating']].values
test_data = test_df[['user_id', 'movie_id', 'rating']].values

epsilon = 10
lambda_ = 100
momentum = 0.8
maxepoch = 20
num_feat = 30
batch_size = 50000
causal = 1

num_p = max(train_df['user_id'].max(), test_df['user_id'].max()) + 1
num_m = max(train_df['movie_id'].max(), test_df['movie_id'].max()) + 1

w1_M1 = np.random.randn(num_m, num_feat + 1)
w1_P1 = np.random.randn(num_p, num_feat + 1)
if causal == 1:
    w1_M1 *= np.sqrt(1 / lambda_)
    w1_P1 *= np.sqrt(1 / lambda_)

w1_M1_inc = np.zeros((num_m, num_feat + 1))
w1_P1_inc = np.zeros((num_p, num_feat + 1))

p_score = np.zeros((num_m, num_p), dtype=np.float32)
items, counts = np.unique(train_data[:, 1], return_counts=True)
p_score[items] = counts[:, None] / num_p

err_train = []
err_valid = []
mean_rating = np.mean(train_data[:, 2])

for epoch in range(maxepoch):
    np.random.shuffle(train_data)
    numbatches = int(np.ceil(len(train_data) / batch_size))

    for batch in range(numbatches):
        start_idx = batch * batch_size
        end_idx = min((batch + 1) * batch_size, len(train_data))
        batch_data = train_data[start_idx:end_idx]

        aa_p = batch_data[:, 0].astype(int)
        aa_m = batch_data[:, 1].astype(int)
        rating = batch_data[:, 2]
        rating = (rating > 3).astype(float)

        pred_out = np.sum(w1_M1[aa_m] * w1_P1[aa_p], axis=1)
        vec_ind = (aa_m, aa_p)
        p_score_vec = p_score[vec_ind]
        pred_out += p_score_vec

        IO = 2 * (pred_out - rating)[:, np.newaxis]
        Ix_m = IO * w1_P1[aa_p] + lambda_ * w1_M1[aa_m]
        Ix_p = IO * w1_M1[aa_m] + lambda_ * w1_P1[aa_p]

        dw1_M1 = np.zeros_like(w1_M1)
        dw1_P1 = np.zeros_like(w1_P1)

        for i in range(len(aa_p)):
            dw1_M1[aa_m[i]] += Ix_m[i]
            dw1_P1[aa_p[i]] += Ix_p[i]

        w1_M1_inc = momentum * w1_M1_inc + epsilon * dw1_M1 / len(aa_p)
        w1_M1 -= w1_M1_inc
        w1_P1_inc = momentum * w1_P1_inc + epsilon * dw1_P1 / len(aa_p)
        w1_P1 -= w1_P1_inc

    pred_out_train = np.sum(w1_M1[train_data[:, 1].astype(int)] * w1_P1[train_data[:, 0].astype(int)], axis=1)
    pred_out_train += p_score[train_data[:, 1].astype(int), train_data[:, 0].astype(int)]
    train_error = np.sqrt(mean_squared_error(train_data[:, 2] > 3, pred_out_train > mean_rating))
    err_train.append(train_error)

    pred_out_test = np.sum(w1_M1[test_data[:, 1].astype(int)] * w1_P1[test_data[:, 0].astype(int)], axis=1)
    pred_out_test += p_score[test_data[:, 1].astype(int), test_data[:, 0].astype(int)]
    valid_error = np.sqrt(mean_squared_error(test_data[:, 2] > 3, pred_out_test > mean_rating))
    err_valid.append(valid_error)

    print(f"Epoch {epoch + 1:4d}: Training RMSE = {train_error:.4f}, Validation RMSE = {valid_error:.4f}")

    if epoch > 1 and err_valid[-2] < err_valid[-1]:
        break

plt.figure()
plt.plot(range(1, len(err_train) + 1), err_train, label='Training error')
plt.plot(range(1, len(err_valid) + 1), err_valid, label='Validation error')
plt.xlabel('Epochs')
plt.ylabel('RMSE')
plt.title(f'Causal PMF - Lr={epsilon:.4f}, lambda={lambda_:.4f}, causal={causal}')
plt.legend()
plt.show()

"""#Causal Probabilistic Matrix Factorization
The code implements Causal Probabilistic Matrix Factorization (PMF) for collaborative filtering. It enhances traditional PMF by incorporating causal initialization and item popularity scores.


The training (train_ratings.csv) and test (test_ratings.csv) datasets are loaded.
Ratings are extracted and stored as numpy arrays for efficient processing.
The number of users (num_p) and movies (num_m) is determined.

User (w1_P1) and movie (w1_M1) feature matrices are initialized with random values.
If causal initialization is enabled (causal = 1), the feature matrices are scaled by 1 / sqrt(lambda_) to improve convergence.
Momentum-based gradient descent variables (w1_M1_inc, w1_P1_inc) are initialized as zero matrices.

A popularity score matrix (p_score) is created to incorporate item popularity as an additional factor.
The number of times each movie appears in the training data is counted.
Popularity scores are normalized by the number of users.

In the Training loop, for each epoch, the following steps are performed:

Data Shuffling and Mini-Batch Processing

The dataset is shuffled to improve model generalization.
The training data is divided into mini-batches.
Forward Pass: Compute Predictions

Predictions are computed using the dot product of user and movie feature vectors.
Popularity scores are added to the predictions.
Backward Pass: Compute Gradients and Update Weights

Gradients of the loss function are computed for user and movie feature matrices.
Stochastic Gradient Descent (SGD) with momentum is used to update weights.

Training RMSE is computed using binary classification (rating > 3 as 1, otherwise 0).
Validation RMSE is computed similarly on the test set.
Early stopping is applied if the validation error increases in consecutive epochs.

The training and validation RMSE values are plotted against epochs using Matplotlib to analyze model performance.
"""