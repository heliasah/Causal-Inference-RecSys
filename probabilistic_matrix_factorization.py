# -*- coding: utf-8 -*-
"""Probabilistic Matrix Factorization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1toE1aZ0O7C9SkaBhmgNCyaALMaPC5_bQ
"""

import numpy as np
import pandas as pd
from sklearn.metrics import mean_squared_error

train_df = pd.read_csv('train_ratings.csv')
test_df = pd.read_csv('test_ratings.csv')

train_data = train_df[['user_id', 'movie_id', 'rating']].values
test_data = test_df[['user_id', 'movie_id', 'rating']].values

epsilon = 10
lambda_ = 0.0001
momentum = 0.8
maxepoch = 50
num_feat = 30
batch_size = 10000

num_p = max(train_df['user_id'].max(), test_df['user_id'].max()) + 1
num_m = max(train_df['movie_id'].max(), test_df['movie_id'].max()) + 1

w1_M1 = 0.1 * np.random.randn(num_m, num_feat)
w1_P1 = 0.1 * np.random.randn(num_p, num_feat)
w1_M1_inc = np.zeros((num_m, num_feat))
w1_P1_inc = np.zeros((num_p, num_feat))

err_train = []
err_valid = []
for epoch in range(maxepoch):
    np.random.shuffle(train_data)
    numbatches = int(np.ceil(len(train_data) / batch_size))

    for batch in range(numbatches):
        start_idx = batch * batch_size
        end_idx = min((batch + 1) * batch_size, len(train_data))
        batch_data = train_data[start_idx:end_idx]

        aa_p = batch_data[:, 0].astype(int)
        aa_m = batch_data[:, 1].astype(int)
        rating = batch_data[:, 2]
        rating = (rating > 3).astype(float)
        mean_rating = np.mean(rating)
        rating = rating - mean_rating

        pred_out = np.sum(w1_M1[aa_m] * w1_P1[aa_p], axis=1)
        f = np.sum((pred_out - rating) ** 2) + 0.5 * lambda_ * (np.sum(w1_M1[aa_m] ** 2) + np.sum(w1_P1[aa_p] ** 2))

        IO = 2 * (pred_out - rating)[:, np.newaxis]
        Ix_m = IO * w1_P1[aa_p] + lambda_ * w1_M1[aa_m]
        Ix_p = IO * w1_M1[aa_m] + lambda_ * w1_P1[aa_p]

        dw1_M1 = np.zeros((num_m, num_feat))
        dw1_P1 = np.zeros((num_p, num_feat))

        for ii in range(len(aa_p)):
            dw1_M1[aa_m[ii]] += Ix_m[ii]
            dw1_P1[aa_p[ii]] += Ix_p[ii]

        w1_M1_inc = momentum * w1_M1_inc + epsilon * dw1_M1 / len(aa_p)
        w1_M1 -= w1_M1_inc
        w1_P1_inc = momentum * w1_P1_inc + epsilon * dw1_P1 / len(aa_p)
        w1_P1 -= w1_P1_inc

    pred_out = np.sum(w1_M1[aa_m] * w1_P1[aa_p], axis=1)
    f_s = np.sum((pred_out - rating) ** 2) + 0.5 * lambda_ * (np.sum(w1_M1[aa_m] ** 2) + np.sum(w1_P1[aa_p] ** 2))
    err_train.append(np.sqrt(f_s / len(aa_p)))

    aa_p_test = test_data[:, 0].astype(int)
    aa_m_test = test_data[:, 1].astype(int)
    rating_test = test_data[:, 2]
    rating_test = (rating_test > 3).astype(float)

    pred_out_test = np.sum(w1_M1[aa_m_test] * w1_P1[aa_p_test], axis=1) + mean_rating
    pred_out_test[pred_out_test > 0] = 1
    pred_out_test[pred_out_test <= 0] = 0

    err_valid.append(np.sqrt(mean_squared_error(rating_test, pred_out_test)))

    print(f"epoch {epoch + 1:4d} batch {batch + 1:4d} Training RMSE {err_train[-1]:6.4f} Test RMSE {err_valid[-1]:6.4f}")

    if epoch > 1 and err_valid[-2] < err_valid[-1]:
        break

import matplotlib.pyplot as plt

plt.figure()
plt.plot(range(1, len(err_train) + 1), err_train, label='Training error')
plt.plot(range(1, len(err_valid) + 1), err_valid, label='Validation error')
plt.xlabel('Epochs')
plt.title(f'Error - Lr={epsilon:.4f}, lambda={lambda_:.4f}, momentum={momentum:.4f}, num_feat={num_feat}')
plt.legend()
plt.show()

"""# Probabilistic Matrix Factorization
The code implements a matrix factorization model for collaborative filtering using gradient descent. It loads preprocessed training and testing datasets containing user-movie rating data. The user and movie interactions are converted into numpy arrays for efficient computation.

The model initializes user and movie feature matrices with small random values and sets hyperparameters such as the learning rate (epsilon), regularization parameter (lambda_), momentum, number of latent factors (num_feat), and batch size.

During training, the dataset is shuffled, and mini-batches are created. For each batch, user and movie IDs are extracted, and ratings are binarized (ratings > 3 are set to 1, otherwise 0) to convert the problem into binary classification. The average rating is subtracted from the ratings to normalize the data.

The model predicts ratings using the dot product of user and movie feature vectors. The loss function is computed as the squared error between predictions and actual ratings, including an L2 regularization term to prevent overfitting. The gradients of the loss function are calculated, and feature matrices are updated using momentum-based stochastic gradient descent (SGD).

After each epoch, the model evaluates performance using Root Mean Squared Error (RMSE) on both training and validation sets. Early stopping is applied if the validation error starts increasing.

Finally, the training and validation errors across epochs are plotted using matplotlib to visualize model performance.
"""